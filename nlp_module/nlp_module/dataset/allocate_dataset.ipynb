{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSe/YudLza0LFq2MF/Of6V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"TXv4VwXCgVSl","executionInfo":{"status":"ok","timestamp":1724605760818,"user_tz":-60,"elapsed":7976,"user":{"displayName":"yue yu","userId":"17383426298810246618"}}},"outputs":[],"source":["import sys\n","import os\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from utils.gen_vocabulary import text_split\n"]},{"cell_type":"code","source":["BASE_DIR = os.path.dirname(__file__)\n","PRJ_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\"))\n","sys.path.append(PRJ_DIR)\n","\n","\n","\n","\n","class WordToIndex(object):\n","    def __init__(self):\n","        self.PAD_TAG = \"PAD\"\n","        self.UNK = 0\n","\n","    def encode(self, sentence, vocab_dict, max_len=None):\n","        if max_len is not None:    # 补齐，切割 句子固定长度\n","            if max_len > len(sentence):\n","                sentence = sentence + [self.PAD_TAG]*(max_len-len(sentence))\n","            if max_len < len(sentence):\n","                sentence = sentence[:max_len]\n","        return [vocab_dict.get(word, self.UNK) for word in sentence]\n","\n","    @staticmethod\n","    def decode(ws_inverse, indices):\n","        return [ws_inverse.get(idx) for idx in indices]\n","\n","\n","class AclImdbDataset(Dataset):\n","    def __init__(self, root_dir, vocab_path, is_train=True, max_len=200):\n","        sub_dir = \"train\" if is_train else \"test\"\n","        self.data_dir = os.path.join(root_dir, sub_dir)\n","        self.vocab_path = vocab_path\n","        self.max_len = max_len\n","\n","        self.word2index = WordToIndex()\n","        self._init_vocab()\n","        self._get_file_info()\n","\n","    def __getitem__(self, item):\n","        # 读取文件路径\n","        file_path = self.total_file_path[item]\n","        # 获取 label\n","        label = 0 if os.path.basename(os.path.dirname(file_path)) == \"neg\" else 1    # neg -> 0; pos -> 1\n","\n","        # tokenize & encode to index\n","        token_list = text_split(open(file_path, encoding='utf-8').read())  # 切分\n","        token_idx_list = self.word2index.encode(token_list, self.vocab, self.max_len)\n","\n","        return np.array(token_idx_list), label\n","\n","    def __len__(self):\n","        return len(self.total_file_path)\n","\n","    def _get_file_info(self):\n","        # 获取所有文件的路径\n","        self.data_dir_list = [os.path.join(self.data_dir, \"pos\"),  os.path.join(self.data_dir, \"neg\")]\n","        self.total_file_path = []\n","        for dir_tmp in self.data_dir_list:\n","            self.file_name_list = os.listdir(dir_tmp)\n","            self.file_path_list = [os.path.join(dir_tmp, file_name) for file_name in self.file_name_list\n","                                   if file_name.endswith(\"txt\")]\n","            self.total_file_path.extend(self.file_path_list)\n","\n","    def _init_vocab(self):\n","        # 加载词表字典\n","        self.vocab = np.load(self.vocab_path, allow_pickle=True).item()\n","\n","\n","if __name__ == \"__main__\":\n","    root_dir = \"\"\n","    vocab_path = os.path.join(BASE_DIR, '..', 'result', \"vocab.npy\")\n","\n","    train_set = AclImdbDataset(root_dir, vocab_path, is_train=True, max_len=200)\n","    valid_set = AclImdbDataset(root_dir, vocab_path, is_train=False, max_len=200)\n","\n","    train_loader = DataLoader(dataset=train_set, batch_size=10, shuffle=True)\n","    for i, (inputs, target) in enumerate(train_loader):\n","        print(i, inputs.shape, inputs, target.shape, target)"],"metadata":{"id":"slOYW88_gdMq"},"execution_count":null,"outputs":[]}]}