{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwGnBmRqFy6YcQC56buoby"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XwXEtUQ2L0KI"},"outputs":[],"source":["import os\n","import sys\n","import random\n","import torch\n","import torch.nn as nn\n","\n","BASE_DIR = os.path.dirname(__file__)\n","PRJ_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\"))\n","sys.path.append(PRJ_DIR)\n","\n","\n","class EncoderLSTM(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n","        super(EncoderLSTM, self).__init__()\n","\n","        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n","        self.hidden_size = hidden_size\n","\n","        # Number of layers in the lstm\n","        self.num_layers = num_layers\n","\n","        # Regularization parameter\n","        self.dropout = nn.Dropout(p)\n","        self.tag = True\n","\n","        # Shape --------------------> (5376, 300) [input size, embedding dims]\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","\n","        # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n","        self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n","\n","    # Shape of x (26, 32) [Sequence_length, batch_size]\n","    def forward(self, x):\n","        # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n","        embedding = self.dropout(self.embedding(x))\n","\n","        # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n","        # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size, hidden_size]\n","        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n","\n","        return hidden_state, cell_state\n","\n","\n","class DecoderLSTM(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n","        super(DecoderLSTM, self).__init__()\n","\n","        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n","        self.hidden_size = hidden_size\n","\n","        # Number of layers in the lstm\n","        self.num_layers = num_layers\n","\n","        # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n","        self.output_size = output_size\n","\n","        # Regularization parameter\n","        self.dropout = nn.Dropout(p)\n","\n","        # Shape --------------------> (5376, 300) [input size, embedding dims]\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","\n","        # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n","        self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n","\n","        # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, hidden_state, cell_state):\n","        # x.shape == batch_size]\n","        x = x.unsqueeze(0)  # x.shape == [1, batch_size]\n","\n","        # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n","        embedding = self.dropout(self.embedding(x))\n","\n","        # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n","        # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n","        outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n","\n","        # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n","        predictions = self.fc(outputs)\n","\n","        # Shape --> predictions (32, 4556) [batch_size , output_size]\n","        predictions = predictions.squeeze(0)\n","\n","        return predictions, hidden_state, cell_state\n","\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, Encoder_LSTM, Decoder_LSTM, device):\n","        super(Seq2Seq, self).__init__()\n","        self.Encoder_LSTM = Encoder_LSTM\n","        self.Decoder_LSTM = Decoder_LSTM\n","        self.device = device\n","\n","    def forward(self, source, target, tfr=0.5):\n","        # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n","        batch_size = source.shape[1]\n","\n","        # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n","        target_len = target.shape[0]\n","        target_vocab_size = self.Decoder_LSTM.output_size\n","\n","        # Shape --> outputs (14, 32, 5766)\n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n","\n","        # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n","        hidden_state, cell_state = self.Encoder_LSTM(source)\n","\n","        # Shape of x (32 elements)\n","        x = target[0]  # <bos> token\n","\n","        for i in range(1, target_len):\n","            # output.shape ==  (bs, vocab_length)\n","            # hidden_state.shape == [num_layers, batch_size size, hidden_size]\n","            output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n","            outputs[i] = output\n","            best_guess = output.argmax(1)  # 0th dimension is batch size, 1st dimension is word embedding\n","            x = target[\n","                i] if random.random() < tfr else best_guess  # Either pass the next word correctly from the dataset or use the earlier predicted word\n","\n","        # Shape --> outputs (14, 32, 5766)\n","        return outputs\n","\n","\n","if __name__ == \"__main__\":\n","    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    device = torch.device(\"cpu\")\n","\n","    # Example usage\n","    SOURCE_vocab_len, TARGET_vocab_len = 8000, 10000\n","    input_dimensions = SOURCE_vocab_len\n","    output_dimensions = TARGET_vocab_len\n","    encoder_embedding_dimensions = 256\n","    decoder_embedding_dimensions = 256\n","    hidden_layer_dimensions = 512\n","    number_of_layers = 2\n","    encoder_dropout = 0.5\n","    decoder_dropout = 0.5\n","\n","    input_size_encoder = SOURCE_vocab_len\n","    encoder_embedding_size = 300\n","    hidden_size = 1024\n","    num_layers = 2\n","    encoder_dropout = 0.5\n","\n","    encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n","                               hidden_size, num_layers, encoder_dropout).to(device)\n","\n","    input_size_decoder = TARGET_vocab_len\n","    decoder_embedding_size = 300\n","    hidden_size = 1024\n","    num_layers = 2\n","    decoder_dropout = 0.5\n","    output_size = TARGET_vocab_len\n","    decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n","                               hidden_size, num_layers, decoder_dropout, output_size).to(device)\n","\n","    model = Seq2Seq(encoder_lstm, decoder_lstm, device).to(device)\n","\n","    batch_size = 16\n","    seq_len = 50\n","\n","    src_fake = torch.randint(0, SOURCE_vocab_len, size=(batch_size, seq_len))\n","    trg_fake = torch.randint(0, TARGET_vocab_len, size=(batch_size, seq_len))\n","    outputs = model(src_fake, trg_fake)\n","\n","    print(outputs.shape)"]}]}