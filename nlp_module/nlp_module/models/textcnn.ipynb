{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOzSqGnTZhb8SMQnfzA6xQ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class TextCNN(nn.Module):\n","    def __init__(self, vocab_size, embed_size, num_filters, filter_sizes, num_classes, embedding_matrix=None):\n","        super(TextCNN, self).__init__()\n","\n","        # Embedding Layer\n","        if embedding_matrix is None:\n","            self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n","        else:\n","            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n","\n","        # Convolutional Layers\n","        self.convs = nn.ModuleList([\n","            nn.Conv1d(in_channels=embed_size, out_channels=num_filters, kernel_size=fs)  #选取不同的卷积核大小提取特证\n","            for fs in filter_sizes\n","        ])\n","\n","        # Fully Connected Layer\n","        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len)\n","\n","        # Embedding Layer output: (batch_size, seq_len, embed_size)\n","        x = self.embedding(x)\n","\n","        # Reshape input for Conv1d: (batch_size, embed_size, seq_len)\n","        x = x.permute(0, 2, 1)\n","\n","        # Apply convolution and pooling\n","        pooled_outputs = []\n","        for conv in self.convs:\n","            conv_out = F.relu(conv(x))  # Conv1d output: (batch_size, num_filters, conv_output_length)\n","            pool_out = F.max_pool1d(conv_out, kernel_size=conv_out.shape[2])  # Max-pooling output: (batch_size, num_filters, 1)\n","            pooled_outputs.append(pool_out.squeeze(2))  # Squeeze to remove last dimension: (batch_size, num_filters)\n","\n","        # Concatenate pooled features\n","        x = torch.cat(pooled_outputs, dim=1)  # Concatenate along num_filters dimension\n","\n","        # Fully connected layer\n","        output = self.fc(x)\n","\n","        return output\n"],"metadata":{"id":"A7Bi_dr4s6wK"},"execution_count":null,"outputs":[]}]}