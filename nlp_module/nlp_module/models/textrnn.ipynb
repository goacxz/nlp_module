{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmmnM73D/HBlihABxAaBbe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tOLNUk7IBZgU"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class TextRNN(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_classes, bidirectional=False):\n","        super(TextRNN, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, bidirectional=bidirectional, batch_first=True)\n","        # 如果是双向 RNN，隐藏层输出维度需要乘以 2\n","        self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)  # (batch_size, seq_len) -> (batch_size, seq_len, embed_size)\n","        out, _ = self.rnn(x)  # (batch_size, seq_len, hidden_size * num_directions)\n","\n","        # 对于双向 RNN，拼接正向和反向隐藏状态\n","        if self.rnn.bidirectional:\n","            out = out[:, -1, :]  # 取最后一个时间步的输出 (batch_size, hidden_size * 2)\n","        else:\n","            out = out[:, -1, :]  # 取最后一个时间步的输出 (batch_size, hidden_size)\n","\n","        out = self.fc(out)  # (batch_size, num_classes)\n","        return out\n","\n"]}]}