{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3oSqh1SXUKchYzOxxXg86"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"osUk_jvJkat5"},"outputs":[],"source":["# model training using rnn& lstm for example\n","import os\n","import sys\n","import time\n","import datetime\n","import numpy as np\n","import torch.optim as optim\n","from torch.utils.tensorboard import SummaryWriter\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","BASE_DIR = os.path.dirname(__file__)\n","PRJ_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\"))\n","sys.path.append(PRJ_DIR)\n","\n","from datasets.allocate_dataset import AclImdbDataset\n","from models.rnn&lstm import RNNTextClassifier, LSTMTextClassifier\n","\n","\n","def get_args_parser(add_help=True):\n","    import argparse\n","\n","    parser = argparse.ArgumentParser(description=\"PyTorch Classification Training\", add_help=add_help)\n","\n","    parser.add_argument(\"--data-path\", default=r\"G:\\deep_learning_data\\aclImdb_v1\\aclImdb\", type=str,\n","                        help=\"dataset path\")\n","    parser.add_argument(\"--glove-file-path\", default=\"G:\\deep_learning_data\\glove.6B\\glove.6B.100d.txt\", type=str,\n","                        help=\"预训练词向量文件\")\n","    parser.add_argument(\"--model-mode\", default=\"lstm\", type=str)\n","    parser.add_argument(\"--device\", default=\"cuda\", type=str)\n","    parser.add_argument(\"-b\", \"--batch-size\", default=64, type=int, help=\"the total batch size is $NGPU x batch_size\")\n","    parser.add_argument(\"--epochs\", default=100, type=int, metavar=\"N\", help=\"number of total epochs to run\")\n","    parser.add_argument(\"-j\", \"--workers\", default=4, type=int, metavar=\"N\", help=\"number of data loading workers\")\n","    parser.add_argument(\"--random-seed\", default=42, type=int, help=\"random seed\")\n","    parser.add_argument(\"--lr\", default=0.01, type=float, help=\"initial learning rate\")\n","    parser.add_argument(\"--momentum\", default=0.9, type=float, metavar=\"M\", help=\"momentum\")\n","    parser.add_argument(\"--lr-step-size\", default=20, type=int, help=\"decrease lr every step-size epochs\")\n","    parser.add_argument(\"--lr-gamma\", default=0.1, type=float, help=\"decrease lr by a factor of lr-gamma\")\n","    parser.add_argument(\"--print-freq\", default=50, type=int, help=\"print frequency\")\n","    parser.add_argument(\"--output-dir\", default=\"./Result\", type=str, help=\"path to save outputs\")\n","    parser.add_argument('--is-freeze', action='store_true', default=False, help='是否冻结embedding层')\n","\n","    return parser\n","\n","\n","if __name__ == \"__main__\":\n","    classes = ['neg', 'pos']\n","    # root_dir = r'G:\\deep_learning_data\\aclImdb_v1\\aclImdb'\n","    result_dir = os.path.join(BASE_DIR, \"result\")\n","    input_size = 100  # embedding size\n","    hidden_size = 128  # hidden state size\n","    num_layers = 2  # RNN层数\n","    text_max_len = 500  # 一个句子token长度\n","    cls_num = 2  # 分类类别\n","\n","    args = get_args_parser().parse_args()\n","    logger, log_dir = utils.make_logger(result_dir)\n","    writer = SummaryWriter(log_dir=log_dir)\n","\n","    # ------------------------------------ step1: dataset ------------------------------------\n","    vocab_path = os.path.join(BASE_DIR, 'result', \"aclImdb_vocab.npy\")  # 通过 a_gen_vocabulary.py 获得\n","\n","    train_set = AclImdbDataset(args.data_path, vocab_path, is_train=True, max_len=text_max_len)\n","    valid_set = AclImdbDataset(args.data_path, vocab_path, is_train=False, max_len=text_max_len)\n","    train_loader = DataLoader(dataset=train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n","    valid_loader = DataLoader(dataset=valid_set, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n","\n","    # ------------------------------------ step2: model ------------------------------------\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if args.model_mode == \"rnn\":\n","        model = RNNTextClassifier(input_size, hidden_size, cls_num, num_layers, vocab_len=len(train_set.vocab),\n","                                  device=device)\n","    elif args.model_mode == \"lstm\":\n","        model = LSTMTextClassifier(len(train_set.vocab), input_size, hidden_size, num_layers)\n","    else:\n","        logger.error(f\"model mode is not recognize! got {args.model_mode}\")\n","    model.apply(utils.init_weights)\n","\n","    if args.glove_file_path:\n","        word2idx = np.load(vocab_path, allow_pickle=True).item()  # 词表顺序仍旧根据训练集统计得到的词表顺序\n","        glove_vectors = utils.load_glove_vectors(args.glove_file_path, word2idx)  # 加载存在的token的vector，不存在的token不加载。\n","        # 将GloVe预训练词向量放到embedding层中\n","        counter = 0\n","        for word, idx in word2idx.items():\n","            if word in glove_vectors:\n","                model.embedding.weight.data[idx] = glove_vectors[word]\n","                counter += 1\n","        # model.embedding.weight.data.copy_(embeds)\n","        if args.is_freeze:\n","            model.embedding.weight.requires_grad = False\n","        logger.info(f'加载了{counter}个预训练词向量.')\n","    model.to(device)\n","\n","    # ------------------------------------ step3: optimizer, lr scheduler ------------------------------------\n","    criterion = nn.CrossEntropyLoss()  # 选择损失函数\n","    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=1e-4)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size,\n","                                                gamma=args.lr_gamma)  # 设置学习率下降策略\n","    # ------------------------------------ step4: iteration ------------------------------------\n","    best_acc, best_epoch = 0, 0\n","    logger.info(args)\n","    logger.info(\"Start training\")\n","    start_time = time.time()\n","    epoch_time_m = utils.AverageMeter()\n","    end = time.time()\n","    for epoch in range(args.epochs):\n","        # 训练\n","        loss_m_train, acc_m_train, mat_train = \\\n","            utils.ModelTrainer.train_one_epoch(train_loader, model, criterion, optimizer, scheduler,\n","                                               epoch, device, args, logger, classes)\n","        # 验证\n","        loss_m_valid, acc_m_valid, mat_valid = \\\n","            utils.ModelTrainer.evaluate(valid_loader, model, criterion, device, classes)\n","\n","        epoch_time_m.update(time.time() - end)\n","        end = time.time()\n","\n","        lr_current = scheduler.get_last_lr()[0]\n","        logger.info(\n","            'Epoch: [{:0>3}/{:0>3}]  '\n","            'Time: {epoch_time.val:.3f} ({epoch_time.avg:.3f})  '\n","            'Train Loss avg: {loss_train.avg:>6.4f}  '\n","            'Valid Loss avg: {loss_valid.avg:>6.4f}  '\n","            'Train Acc@1 avg:  {top1_train.avg:>7.4f}   '\n","            'Valid Acc@1 avg: {top1_valid.avg:>7.4f}    '\n","            'LR: {lr}'.format(\n","                epoch, args.epochs, epoch_time=epoch_time_m, loss_train=loss_m_train, loss_valid=loss_m_valid,\n","                top1_train=acc_m_train, top1_valid=acc_m_valid, lr=lr_current))\n","\n","        # 学习率更新\n","        scheduler.step()\n","\n","        # 记录\n","        writer.add_scalars('Loss_group', {'train_loss': loss_m_train.avg,\n","                                          'valid_loss': loss_m_valid.avg}, epoch)\n","        writer.add_scalars('Accuracy_group', {'train_acc': acc_m_train.avg,\n","                                              'valid_acc': acc_m_valid.avg}, epoch)\n","        conf_mat_figure_train = utils.show_conf_mat(mat_train, classes, \"train\", log_dir, epoch=epoch,\n","                                                    verbose=epoch == args.epochs - 1, save=True)\n","        conf_mat_figure_valid = utils.show_conf_mat(mat_valid, classes, \"valid\", log_dir, epoch=epoch,\n","                                                    verbose=epoch == args.epochs - 1, save=True)\n","        writer.add_figure('confusion_matrix_train', conf_mat_figure_train, global_step=epoch)\n","        writer.add_figure('confusion_matrix_valid', conf_mat_figure_valid, global_step=epoch)\n","        writer.add_scalar('learning rate', lr_current, epoch)\n","\n","        # ------------------------------------ 模型保存 ------------------------------------\n","        if best_acc < acc_m_valid.avg or epoch == args.epochs - 1:\n","            best_epoch = epoch if best_acc < acc_m_valid.avg else best_epoch\n","            best_acc = acc_m_valid.avg if best_acc < acc_m_valid.avg else best_acc\n","            checkpoint = {\n","                \"model_state_dict\": model.state_dict(),\n","                \"optimizer_state_dict\": optimizer.state_dict(),\n","                \"lr_scheduler_state_dict\": scheduler.state_dict(),\n","                \"epoch\": epoch,\n","                \"args\": args,\n","                \"best_acc\": best_acc}\n","            pkl_name = \"checkpoint_{}.pth\".format(epoch) if epoch == args.epochs - 1 else \"checkpoint_best.pth\"\n","            path_checkpoint = os.path.join(log_dir, pkl_name)\n","            torch.save(checkpoint, path_checkpoint)\n","            logger.info(f'save ckpt done! best acc:{best_acc}, epoch:{epoch}')\n","\n","    total_time = time.time() - start_time\n","    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n","    logger.info(\"Training time {}\".format(total_time_str))"]}]}